{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "In this lab we will implement and train an agent that uses deep learning to play balance the stick in `CartPole-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "----\n",
    "We import useful packages: `gym`, `torch` stuff, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque  # for memory\n",
    "from tqdm import tqdm          # for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How the game looks (without our agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "for _ in tqdm(range(10)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DQN Algorithm\n",
    "-------------\n",
    "We train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is *return*. The discount, $\\gamma$ is the discount, between $0$ and $1$\n",
    "\n",
    "\n",
    "Q-learning tries to find a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, maximizes rewards:\n",
    "\n",
    "$ \\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align} $\n",
    "\n",
    "However, we don't know $ Q^* $. So, we use neural network as a approximators, we can simply create one and train it to resemble $ Q^* $.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$ \\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align} $\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
    "\n",
    "$ \\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model\n",
    "---\n",
    "Make a deep learning based policy model, that takes in a state and outputs an action.\n",
    "This model will be an attribute of the Agent we make next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, hidden_size=100):\n",
    "        super(Model, self).__init__()\n",
    "        # initialise layers here\n",
    "        self.dense_layer1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.dense_layer2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # send x through the network\n",
    "        x = self.dense_layer1(x)\n",
    "        x = self.dense_layer2(x)\n",
    "        return x\n",
    "\n",
    "    # This is to have a function that outputs an action with the highest q-value\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DQN Agent\n",
    "----\n",
    "We will be using experience replay memory for training our model.\n",
    "An Agent's memory is as important as its model, and will be another attribute of our agent.\n",
    "Other appropriate attributes are the hyperparameters (gamma, lr, etc.).\n",
    "Give the agent a replay method that trains on a batch from its memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import queue\n",
    "import random\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        self.observation_size=observation_size\n",
    "        self.action_size = action_size\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model = Model(observation_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.memory = [] # memory that stores N most new transitions\n",
    "        # good place to store hyperparameters as attributes\n",
    "        self.learning_rate = 0.7           # Learning rate\n",
    "        self.gamma = 0.9                   # Discounting rate - Old value: 0.618\n",
    "\n",
    "        self.total_episodes = 5000        # Total episodes\n",
    "        # total_test_episodes = 5       # Total test episodes\n",
    "        # max_steps = 99                # Max steps per episode\n",
    "\n",
    "        # # Exploration parameters\n",
    "        self.epsilon = 1.0                 # Exploration rate\n",
    "        # max_epsilon = 1.0             # Exploration probability at start\n",
    "        # min_epsilon = 0.01            # Minimum exploration probability\n",
    "        # decay_rate = 0.0001             # Exponential decay rate for exploration prob\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # add to memory\n",
    "\n",
    "        print(state)\n",
    "        print(action)\n",
    "        print(reward)\n",
    "        print(next_state)\n",
    "        \n",
    "        transition = (state, action, next_state, reward, done)\n",
    "        if len(self.memory) > 10000:\n",
    "            del self.memory[0]\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory.append(transition)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        # return an action from the model\n",
    "        # Doing a random choice --> exploration\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        # Else --> exploitation (taking the biggest Q value for this state)\n",
    "        else:\n",
    "            action = torch.argmax(self.model.predict(torch.tensor(state)))\n",
    "            action = int(action)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # update model based on replay memory\n",
    "        # you might want to make a self.train() helper method\n",
    "        transitions = self.sample(batch_size)\n",
    "        for state, action, reward, next_state, done in transitions:\n",
    "\n",
    "            q_current = self.model.forward(torch.tensor(state, dtype=torch.float32))[action]\n",
    "            #self.model.forward(torch.tensor(state))\n",
    "            #float(torch.max(self.model.forward(torch.tensor(state))))\n",
    "            #self.model.forward(torch.tensor(state))[action]\n",
    "            q_target = reward + (torch.max(self.model.forward(torch.tensor(next_state, dtype=torch.float32))) * int(not done))\n",
    "            # float(torch.max(self.model.forward(torch.tensor(state))))\n",
    "            \n",
    "            #value = float(torch.max(self.model.forward(torch.tensor(state))))\n",
    "\n",
    "            loss = self.criterion(q_current, q_target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(not False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Training loop\n",
    "---\n",
    "Make a function that takes and environment and an agent, and runs through $n$ episodes.\n",
    "Remember to call that agent's replay function to learn from its past (once it has a past).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(env, agent, episodes=1000, batch_size=64):  # train for many games\n",
    "    for _ in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            # 2. have the agent remember stuff.\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # 3. update state\n",
    "            state = next_state\n",
    "            # 4. if we have enough experiences in out memory, learn from a batch with replay.\n",
    "            if len(agent.memory) >= batch_size:\n",
    "                agent.replay(batch_size)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Putting it together\n",
    "---\n",
    "We train an agent on the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<00:07, 125.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01417059 -0.00935043  0.01676764 -0.03222782]\n",
      "0\n",
      "1.0\n",
      "[-0.0143576  -0.20470877  0.01612308  0.26569796]\n",
      "[-0.0143576  -0.20470877  0.01612308  0.26569796]\n",
      "0\n",
      "1.0\n",
      "[-0.01845177 -0.40005708  0.02143704  0.5634223 ]\n",
      "[-0.01845177 -0.40005708  0.02143704  0.5634223 ]\n",
      "0\n",
      "1.0\n",
      "[-0.02645291 -0.5954732   0.03270549  0.8627811 ]\n",
      "[-0.02645291 -0.5954732   0.03270549  0.8627811 ]\n",
      "1\n",
      "1.0\n",
      "[-0.03836238 -0.4008114   0.04996111  0.5805583 ]\n",
      "[-0.03836238 -0.4008114   0.04996111  0.5805583 ]\n",
      "0\n",
      "1.0\n",
      "[-0.04637861 -0.59659654  0.06157227  0.888552  ]\n",
      "[-0.04637861 -0.59659654  0.06157227  0.888552  ]\n",
      "1\n",
      "1.0\n",
      "[-0.05831053 -0.4023618   0.07934331  0.6158428 ]\n",
      "[-0.05831053 -0.4023618   0.07934331  0.6158428 ]\n",
      "1\n",
      "1.0\n",
      "[-0.06635777 -0.20843281  0.09166017  0.34916696]\n",
      "[-0.06635777 -0.20843281  0.09166017  0.34916696]\n",
      "0\n",
      "1.0\n",
      "[-0.07052643 -0.4047307   0.09864351  0.66928816]\n",
      "[-0.07052643 -0.4047307   0.09864351  0.66928816]\n",
      "0\n",
      "1.0\n",
      "[-0.07862104 -0.6010758   0.11202927  0.99132824]\n",
      "[-0.07862104 -0.6010758   0.11202927  0.99132824]\n",
      "1\n",
      "1.0\n",
      "[-0.09064256 -0.40761685  0.13185583  0.7358267 ]\n",
      "[-0.09064256 -0.40761685  0.13185583  0.7358267 ]\n",
      "1\n",
      "1.0\n",
      "[-0.0987949  -0.21453883  0.14657237  0.487377  ]\n",
      "[-0.0987949  -0.21453883  0.14657237  0.487377  ]\n",
      "1\n",
      "1.0\n",
      "[-0.10308567 -0.021756    0.15631992  0.24424157]\n",
      "[-0.10308567 -0.021756    0.15631992  0.24424157]\n",
      "0\n",
      "1.0\n",
      "[-0.1035208  -0.21872509  0.16120474  0.5818638 ]\n",
      "[-0.1035208  -0.21872509  0.16120474  0.5818638 ]\n",
      "0\n",
      "1.0\n",
      "[-0.10789529 -0.41569477  0.17284201  0.9206769 ]\n",
      "[-0.10789529 -0.41569477  0.17284201  0.9206769 ]\n",
      "1\n",
      "1.0\n",
      "[-0.11620919 -0.22327678  0.19125555  0.6869133 ]\n",
      "[-0.11620919 -0.22327678  0.19125555  0.6869133 ]\n",
      "0\n",
      "1.0\n",
      "[-0.12067473 -0.42046633  0.20499383  1.0331914 ]\n",
      "[-0.12067473 -0.42046633  0.20499383  1.0331914 ]\n",
      "1\n",
      "1.0\n",
      "[-0.12908405 -0.22857244  0.22565766  0.8112243 ]\n",
      "[-0.04851884  0.03080463 -0.00910153 -0.01845185]\n",
      "1\n",
      "1.0\n",
      "[-0.04790274  0.22605592 -0.00947057 -0.31399247]\n",
      "[-0.04790274  0.22605592 -0.00947057 -0.31399247]\n",
      "1\n",
      "1.0\n",
      "[-0.04338163  0.4213115  -0.01575042 -0.609647  ]\n",
      "[-0.04338163  0.4213115  -0.01575042 -0.609647  ]\n",
      "0\n",
      "1.0\n",
      "[-0.0349554   0.2264132  -0.02794336 -0.32196626]\n",
      "[-0.0349554   0.2264132  -0.02794336 -0.32196626]\n",
      "0\n",
      "1.0\n",
      "[-0.03042713  0.03170009 -0.03438269 -0.0382249 ]\n",
      "[-0.03042713  0.03170009 -0.03438269 -0.0382249 ]\n",
      "1\n",
      "1.0\n",
      "[-0.02979313  0.2272978  -0.03514718 -0.34155455]\n",
      "[-0.02979313  0.2272978  -0.03514718 -0.34155455]\n",
      "0\n",
      "1.0\n",
      "[-0.02524718  0.03269307 -0.04197827 -0.0601589 ]\n",
      "[-0.02524718  0.03269307 -0.04197827 -0.0601589 ]\n",
      "0\n",
      "1.0\n",
      "[-0.02459331 -0.16180263 -0.04318145  0.21898967]\n",
      "[-0.02459331 -0.16180263 -0.04318145  0.21898967]\n",
      "1\n",
      "1.0\n",
      "[-0.02782937  0.03390912 -0.03880166 -0.08699573]\n",
      "[-0.02782937  0.03390912 -0.03880166 -0.08699573]\n",
      "1\n",
      "1.0\n",
      "[-0.02715118  0.22956516 -0.04054157 -0.3916637 ]\n",
      "[-0.02715118  0.22956516 -0.04054157 -0.3916637 ]\n",
      "0\n",
      "1.0\n",
      "[-0.02255988  0.03504131 -0.04837485 -0.11203365]\n",
      "[-0.02255988  0.03504131 -0.04837485 -0.11203365]\n",
      "0\n",
      "1.0\n",
      "[-0.02185905 -0.15935528 -0.05061552  0.16500348]\n",
      "[-0.02185905 -0.15935528 -0.05061552  0.16500348]\n",
      "0\n",
      "1.0\n",
      "[-0.02504616 -0.3537175  -0.04731545  0.44129884]\n",
      "[-0.02504616 -0.3537175  -0.04731545  0.44129884]\n",
      "1\n",
      "1.0\n",
      "[-0.03212051 -0.15795904 -0.03848948  0.13408418]\n",
      "[-0.03212051 -0.15795904 -0.03848948  0.13408418]\n",
      "0\n",
      "1.0\n",
      "[-0.03527969 -0.35250914 -0.03580779  0.41438007]\n",
      "[-0.03527969 -0.35250914 -0.03580779  0.41438007]\n",
      "1\n",
      "1.0\n",
      "[-0.04232987 -0.15689842 -0.02752019  0.11062688]\n",
      "[-0.04232987 -0.15689842 -0.02752019  0.11062688]\n",
      "1\n",
      "1.0\n",
      "[-0.04546784  0.03860684 -0.02530765 -0.1906099 ]\n",
      "[-0.04546784  0.03860684 -0.02530765 -0.1906099 ]\n",
      "1\n",
      "1.0\n",
      "[-0.04469571  0.23408154 -0.02911985 -0.4911677 ]\n",
      "[-0.04469571  0.23408154 -0.02911985 -0.4911677 ]\n",
      "1\n",
      "1.0\n",
      "[-0.04001407  0.42960188 -0.03894321 -0.79288393]\n",
      "[-0.04001407  0.42960188 -0.03894321 -0.79288393]\n",
      "0\n",
      "1.0\n",
      "[-0.03142204  0.23503558 -0.05480088 -0.51270217]\n",
      "[-0.03142204  0.23503558 -0.05480088 -0.51270217]\n",
      "1\n",
      "1.0\n",
      "[-0.02672132  0.43088484 -0.06505492 -0.8221384 ]\n",
      "[-0.02672132  0.43088484 -0.06505492 -0.8221384 ]\n",
      "0\n",
      "1.0\n",
      "[-0.01810363  0.23671041 -0.08149769 -0.55060554]\n",
      "[-0.01810363  0.23671041 -0.08149769 -0.55060554]\n",
      "1\n",
      "1.0\n",
      "[-0.01336942  0.4328768  -0.09250981 -0.8678123 ]\n",
      "[-0.01336942  0.4328768  -0.09250981 -0.8678123 ]\n",
      "1\n",
      "1.0\n",
      "[-0.00471188  0.6291275  -0.10986605 -1.1880887 ]\n",
      "[-0.00471188  0.6291275  -0.10986605 -1.1880887 ]\n",
      "1\n",
      "1.0\n",
      "[ 0.00787067  0.82548875 -0.13362782 -1.5130904 ]\n",
      "[ 0.00787067  0.82548875 -0.13362782 -1.5130904 ]\n",
      "1\n",
      "1.0\n",
      "[ 0.02438044  1.0219516  -0.16388963 -1.8443273 ]\n",
      "[ 0.02438044  1.0219516  -0.16388963 -1.8443273 ]\n",
      "0\n",
      "1.0\n",
      "[ 0.04481947  0.82897264 -0.20077617 -1.6067058 ]\n",
      "[ 0.04481947  0.82897264 -0.20077617 -1.6067058 ]\n",
      "1\n",
      "1.0\n",
      "[ 0.06139892  1.0258222  -0.23291029 -1.954681  ]\n",
      "[-0.04857275  0.04127489  0.01891308 -0.02675823]\n",
      "1\n",
      "1.0\n",
      "[-0.04774725  0.23612057  0.01837792 -0.31341437]\n",
      "[-0.04774725  0.23612057  0.01837792 -0.31341437]\n",
      "1\n",
      "1.0\n",
      "[-0.04302484  0.43097597  0.01210963 -0.6002453 ]\n",
      "[-0.04302484  0.43097597  0.01210963 -0.6002453 ]\n",
      "1\n",
      "1.0\n",
      "[-3.44053172e-02  6.25926435e-01  1.04725055e-04 -8.89089406e-01]\n",
      "[-3.44053172e-02  6.25926435e-01  1.04725055e-04 -8.89089406e-01]\n",
      "0\n",
      "1.0\n",
      "[-0.02188679  0.43080306 -0.01767706 -0.59637356]\n",
      "[-0.02188679  0.43080306 -0.01767706 -0.59637356]\n",
      "0\n",
      "1.0\n",
      "[-0.01327073  0.23593289 -0.02960453 -0.30931076]\n",
      "[-0.01327073  0.23593289 -0.02960453 -0.30931076]\n",
      "0\n",
      "1.0\n",
      "[-0.00855207  0.04124498 -0.03579075 -0.02610933]\n",
      "[-0.00855207  0.04124498 -0.03579075 -0.02610933]\n",
      "0\n",
      "1.0\n",
      "[-0.00772717 -0.15334591 -0.03631294  0.25506985]\n",
      "[-0.00772717 -0.15334591 -0.03631294  0.25506985]\n",
      "1\n",
      "1.0\n",
      "[-0.01079409  0.04227519 -0.03121154 -0.04884203]\n",
      "[-0.01079409  0.04227519 -0.03121154 -0.04884203]\n",
      "1\n",
      "1.0\n",
      "[-0.00994859  0.23783046 -0.03218838 -0.35120678]\n",
      "[-0.00994859  0.23783046 -0.03218838 -0.35120678]\n",
      "0\n",
      "1.0\n",
      "[-0.00519198  0.0431807  -0.03921251 -0.06884512]\n",
      "[-0.00519198  0.0431807  -0.03921251 -0.06884512]\n",
      "0\n",
      "1.0\n",
      "[-0.00432836 -0.15135777 -0.04058942  0.21121274]\n",
      "[-0.00432836 -0.15135777 -0.04058942  0.21121274]\n",
      "0\n",
      "1.0\n",
      "[-0.00735552 -0.34587657 -0.03636516  0.49082062]\n",
      "[-0.00735552 -0.34587657 -0.03636516  0.49082062]\n",
      "0\n",
      "1.0\n",
      "[-0.01427305 -0.5404672  -0.02654875  0.77182454]\n",
      "[-0.01427305 -0.5404672  -0.02654875  0.77182454]\n",
      "0\n",
      "1.0\n",
      "[-0.02508239 -0.73521394 -0.01111226  1.0560373 ]\n",
      "[-0.02508239 -0.73521394 -0.01111226  1.0560373 ]\n",
      "1\n",
      "1.0\n",
      "[-0.03978667 -0.5399465   0.01000849  0.75988734]\n",
      "[-0.03978667 -0.5399465   0.01000849  0.75988734]\n",
      "1\n",
      "1.0\n",
      "[-0.0505856  -0.34496388  0.02520623  0.47037047]\n",
      "[-0.0505856  -0.34496388  0.02520623  0.47037047]\n",
      "0\n",
      "1.0\n",
      "[-0.05748488 -0.54043263  0.03461364  0.7708903 ]\n",
      "[-0.05748488 -0.54043263  0.03461364  0.7708903 ]\n",
      "1\n",
      "1.0\n",
      "[-0.06829353 -0.34580368  0.05003145  0.48929614]\n",
      "[-0.06829353 -0.34580368  0.05003145  0.48929614]\n",
      "0\n",
      "1.0\n",
      "[-0.0752096  -0.54159445  0.05981737  0.7973179 ]\n",
      "[-0.0752096  -0.54159445  0.05981737  0.7973179 ]\n",
      "1\n",
      "1.0\n",
      "[-0.0860415  -0.34734204  0.07576373  0.5240363 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adobe\\Desktop\\Skolle\\Msc-3-semester-Games\\DeepLearning\\dlgs\\labs\\lab5.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# , render_mode='human')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(env, agent)\n",
      "\u001b[1;32mc:\\Users\\adobe\\Desktop\\Skolle\\Msc-3-semester-Games\\DeepLearning\\dlgs\\labs\\lab5.ipynb Cell 16\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, episodes, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# 4. if we have enough experiences in out memory, learn from a batch with replay.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32mc:\\Users\\adobe\\Desktop\\Skolle\\Msc-3-semester-Games\\DeepLearning\\dlgs\\labs\\lab5.ipynb Cell 16\u001b[0m in \u001b[0;36mAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m q_current \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))[action]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m#self.model.forward(torch.tensor(state))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m#float(torch.max(self.model.forward(torch.tensor(state))))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m#self.model.forward(torch.tensor(state))[action]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m q_target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m (torch\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(torch\u001b[39m.\u001b[39;49mtensor(next_state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))) \u001b[39m*\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mnot\u001b[39;00m done))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# float(torch.max(self.model.forward(torch.tensor(state))))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m#value = float(torch.max(self.model.forward(torch.tensor(state))))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(q_current, q_target)\n",
      "\u001b[1;32mc:\\Users\\adobe\\Desktop\\Skolle\\Msc-3-semester-Games\\DeepLearning\\dlgs\\labs\\lab5.ipynb Cell 16\u001b[0m in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# send x through the network\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense_layer1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_layer2(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adobe/Desktop/Skolle/Msc-3-semester-Games/DeepLearning/dlgs/labs/lab5.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # , render_mode='human')\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "train(env, agent)\n",
    "# torch.save(agent.model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional (highly recommended): Atari\n",
    "Adapt your agent to play an Atari game of your choice.\n",
    "https://www.gymlibrary.dev/environments/atari/air_raid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca90a0b97817074c741e4139491e51fe8c81a6af5348c47037bc5072db26432e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
